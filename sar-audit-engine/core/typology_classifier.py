from __future__ import annotations

import argparse
import json
import re
from collections import Counter
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence, Tuple

from config.settings import PROCESSED_DIR, PipelineSettings

from .feature_extractor import extract_signals

_BEGIN_PATTERN = re.compile(r"^BEGIN LAUNDERING ATTEMPT\s*-\s*(.*?)(?::\s|$)")

# Backwards-compatible repair for labels generated by the older parser.
_ALIASES = {
    "IN": "FAN-IN",
    "OUT": "FAN-OUT",
    "GATHER": "SCATTER-GATHER",
    "SCATTER": "GATHER-SCATTER",
}

FEATURE_NAMES: List[str] = [
    "tx_count",
    "total_inflow",
    "total_outflow",
    "abs_net_flow_ratio",
    "unique_senders",
    "unique_receivers",
    "hop_count",
    "cross_bank_ratio",
    "multi_currency_flag",
    "rapid_pass_through_flag",
    "rapid_pass_through_min_hours",
    "duration_hours",
    "tx_per_day",
    "avg_interarrival_minutes",
    "median_interarrival_minutes",
    "max_tx_in_1h",
    "payment_format_count",
    "currency_count",
    "avg_amount_received",
    "max_amount_received",
]


def _extract_case_number(case_id_or_name: str) -> int:
    match = re.search(r"(\d+)", case_id_or_name or "")
    return int(match.group(1)) if match else 0


def _canonical_typology(label: object) -> str:
    cleaned = str(label or "").strip().upper().replace("_", "-")
    cleaned = re.sub(r"\s+", " ", cleaned)
    return _ALIASES.get(cleaned, cleaned)


def parse_pattern_typology_map(patterns_path: Path) -> Dict[str, str]:
    mapping: Dict[str, str] = {}
    case_counter = 1

    with Path(patterns_path).open("r", encoding="utf-8") as handle:
        for raw_line in handle:
            line = raw_line.strip()
            match = _BEGIN_PATTERN.match(line)
            if not match:
                continue
            typology = _canonical_typology(match.group(1))
            mapping[f"CASE_{case_counter:03d}"] = typology
            case_counter += 1
    return mapping


def _safe_float(value: object, default: float = 0.0) -> float:
    try:
        if value is None:
            return default
        return float(value)
    except (TypeError, ValueError):
        return default


def _feature_vector(case: Dict[str, Any]) -> Dict[str, float]:
    signals_payload = extract_signals(case)
    signals = signals_payload.get("signals", {}) or {}
    velocity = signals.get("transaction_velocity_metrics", {}) or {}

    transactions = case.get("enriched_transactions") or case.get("transactions") or []
    payment_formats = {
        str(tx.get("payment_format"))
        for tx in transactions
        if tx.get("payment_format") not in (None, "")
    }
    currencies = set()
    amount_received_values: List[float] = []
    for tx in transactions:
        receiving_currency = tx.get("receiving_currency")
        payment_currency = tx.get("payment_currency")
        if receiving_currency not in (None, ""):
            currencies.add(str(receiving_currency))
        if payment_currency not in (None, ""):
            currencies.add(str(payment_currency))
        amount_received_values.append(_safe_float(tx.get("amount_received"), default=0.0))

    total_inflow = _safe_float(signals.get("total_inflow"), default=0.0)
    net_flow = _safe_float(signals.get("net_flow"), default=0.0)
    abs_net_flow_ratio = abs(net_flow) / total_inflow if total_inflow > 0 else 0.0

    avg_amount_received = (
        sum(amount_received_values) / len(amount_received_values) if amount_received_values else 0.0
    )
    max_amount_received = max(amount_received_values) if amount_received_values else 0.0

    feature_map = {
        "tx_count": _safe_float(velocity.get("tx_count")),
        "total_inflow": total_inflow,
        "total_outflow": _safe_float(signals.get("total_outflow")),
        "abs_net_flow_ratio": abs_net_flow_ratio,
        "unique_senders": _safe_float(signals.get("unique_senders")),
        "unique_receivers": _safe_float(signals.get("unique_receivers")),
        "hop_count": _safe_float(signals.get("hop_count")),
        "cross_bank_ratio": _safe_float(signals.get("cross_bank_ratio")),
        "multi_currency_flag": float(bool(signals.get("multi_currency_flag"))),
        "rapid_pass_through_flag": float(bool(signals.get("rapid_pass_through_flag"))),
        "rapid_pass_through_min_hours": _safe_float(signals.get("rapid_pass_through_min_hours"), default=0.0),
        "duration_hours": _safe_float(velocity.get("duration_hours"), default=0.0),
        "tx_per_day": _safe_float(velocity.get("tx_per_day"), default=0.0),
        "avg_interarrival_minutes": _safe_float(velocity.get("avg_interarrival_minutes"), default=0.0),
        "median_interarrival_minutes": _safe_float(velocity.get("median_interarrival_minutes"), default=0.0),
        "max_tx_in_1h": _safe_float(velocity.get("max_tx_in_1h"), default=0.0),
        "payment_format_count": float(len(payment_formats)),
        "currency_count": float(len(currencies)),
        "avg_amount_received": avg_amount_received,
        "max_amount_received": max_amount_received,
    }
    return feature_map


def _matrix_rows(feature_rows: Sequence[Dict[str, float]]) -> List[List[float]]:
    return [[float(row.get(name, 0.0)) for name in FEATURE_NAMES] for row in feature_rows]


def _load_case(path: Path) -> Dict[str, Any]:
    with Path(path).open("r", encoding="utf-8") as handle:
        return json.load(handle)


def _load_training_rows(
    cases_dir: Path,
    patterns_path: Optional[Path],
    min_transactions: int,
) -> Tuple[List[Dict[str, float]], List[str], Dict[str, int]]:
    case_files = sorted(
        Path(cases_dir).glob("*.json"),
        key=lambda path: _extract_case_number(path.stem),
    )
    pattern_typology_map = parse_pattern_typology_map(patterns_path) if patterns_path else {}

    features: List[Dict[str, float]] = []
    labels: List[str] = []
    skipped = {"unknown_label": 0, "too_few_transactions": 0}

    for case_file in case_files:
        case = _load_case(case_file)
        txs = case.get("enriched_transactions") or case.get("transactions") or []
        if len(txs) < int(min_transactions):
            skipped["too_few_transactions"] += 1
            continue

        case_id = str(case.get("case_id") or case_file.stem)
        typology = pattern_typology_map.get(case_id)
        if not typology:
            typology = _canonical_typology(case.get("typology"))
        if not typology:
            skipped["unknown_label"] += 1
            continue

        features.append(_feature_vector(case))
        labels.append(typology)

    return features, labels, skipped


def train_typology_classifier(
    cases_dir: Path,
    model_output_path: Path,
    patterns_path: Path | None = None,
    expected_types: int = 10,
    test_size: float = 0.2,
    random_state: int = 42,
    min_transactions: int = 1,
) -> Dict[str, Any]:
    try:
        import joblib
        from sklearn.impute import SimpleImputer
        from sklearn.linear_model import LogisticRegression
        from sklearn.metrics import accuracy_score, f1_score
        from sklearn.model_selection import train_test_split
        from sklearn.pipeline import Pipeline
        from sklearn.preprocessing import StandardScaler
    except ImportError as exc:
        raise RuntimeError(
            "scikit-learn and joblib are required for typology model training. "
            "Install with: pip install scikit-learn joblib"
        ) from exc

    feature_rows, labels, skipped = _load_training_rows(
        cases_dir=Path(cases_dir),
        patterns_path=Path(patterns_path) if patterns_path else None,
        min_transactions=int(min_transactions),
    )
    if len(feature_rows) < 20:
        raise ValueError("Not enough training samples. Expected at least 20 labeled cases.")

    label_distribution = dict(sorted(Counter(labels).items()))
    if len(label_distribution) < 2:
        raise ValueError(f"Need at least two typology classes. Found: {label_distribution}")

    x = _matrix_rows(feature_rows)
    y = labels
    try:
        x_train, x_test, y_train, y_test = train_test_split(
            x,
            y,
            test_size=float(test_size),
            random_state=int(random_state),
            stratify=y,
        )
    except ValueError:
        x_train, x_test, y_train, y_test = train_test_split(
            x,
            y,
            test_size=float(test_size),
            random_state=int(random_state),
            stratify=None,
        )

    model = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
            (
                "clf",
                LogisticRegression(
                    max_iter=1200,
                    class_weight="balanced",
                ),
            ),
        ]
    )
    model.fit(x_train, y_train)

    predictions = model.predict(x_test)
    accuracy = float(accuracy_score(y_test, predictions))
    macro_f1 = float(f1_score(y_test, predictions, average="macro"))

    top3_accuracy = None
    if hasattr(model, "predict_proba"):
        proba = model.predict_proba(x_test)
        classes = list(model.classes_)
        hits = 0
        for row_proba, true_label in zip(proba, y_test):
            ranked = sorted(
                zip(classes, row_proba),
                key=lambda item: item[1],
                reverse=True,
            )
            top_labels = [label for label, _ in ranked[: min(3, len(ranked))]]
            if true_label in top_labels:
                hits += 1
        top3_accuracy = float(hits / len(y_test)) if y_test else None

    model_output_path = Path(model_output_path)
    model_output_path.parent.mkdir(parents=True, exist_ok=True)
    artifact = {
        "model": model,
        "feature_names": FEATURE_NAMES,
        "patterns_path": str(patterns_path) if patterns_path else None,
    }
    joblib.dump(artifact, model_output_path)

    warnings: List[str] = []
    observed_classes = sorted(label_distribution.keys())
    if expected_types and len(observed_classes) != int(expected_types):
        warnings.append(
            f"Observed {len(observed_classes)} typology classes in training data; "
            f"expected {int(expected_types)}."
        )

    metadata = {
        "accuracy": round(accuracy, 6),
        "macro_f1": round(macro_f1, 6),
        "top3_accuracy": round(top3_accuracy, 6) if top3_accuracy is not None else None,
        "train_samples": len(x_train),
        "test_samples": len(x_test),
        "classes": observed_classes,
        "class_count": len(observed_classes),
        "label_distribution": label_distribution,
        "skipped_rows": skipped,
        "expected_types": int(expected_types),
        "feature_names": FEATURE_NAMES,
        "model_path": str(model_output_path),
        "warnings": warnings,
    }
    metadata_path = model_output_path.with_suffix(model_output_path.suffix + ".meta.json")
    metadata_path.write_text(json.dumps(metadata, indent=2), encoding="utf-8")
    return metadata


def predict_typology_for_case(
    case_payload: Dict[str, Any],
    model_path: Path,
    top_k: int = 3,
) -> Dict[str, Any]:
    try:
        import joblib
    except ImportError as exc:
        raise RuntimeError("joblib is required for prediction. Install with: pip install joblib") from exc

    loaded = joblib.load(Path(model_path))
    if isinstance(loaded, dict) and "model" in loaded:
        model = loaded["model"]
    else:
        model = loaded

    vector = _matrix_rows([_feature_vector(case_payload)])[0]
    prediction = str(model.predict([vector])[0])

    ranked: List[Dict[str, Any]] = [{"label": prediction, "score": None}]
    if hasattr(model, "predict_proba"):
        proba = model.predict_proba([vector])[0]
        classes = list(model.classes_)
        ranked_pairs = sorted(
            zip(classes, proba),
            key=lambda item: item[1],
            reverse=True,
        )[: max(1, int(top_k))]
        ranked = [{"label": label, "score": float(score)} for label, score in ranked_pairs]

    return {
        "predicted_typology": prediction,
        "top_predictions": ranked,
    }


def _train_cmd(args: argparse.Namespace) -> None:
    metrics = train_typology_classifier(
        cases_dir=args.cases_dir,
        model_output_path=args.model_output,
        patterns_path=args.patterns_path,
        expected_types=args.expected_types,
        test_size=args.test_size,
        random_state=args.random_state,
        min_transactions=args.min_transactions,
    )
    print(json.dumps(metrics, indent=2))


def _predict_cmd(args: argparse.Namespace) -> None:
    with Path(args.case_json).open("r", encoding="utf-8") as handle:
        case_payload = json.load(handle)
    prediction = predict_typology_for_case(
        case_payload=case_payload,
        model_path=args.model_path,
        top_k=args.top_k,
    )
    print(json.dumps(prediction, indent=2))


def parse_args() -> argparse.Namespace:
    pipeline = PipelineSettings()
    default_model_path = PROCESSED_DIR / "models" / "audit_typology_classifier.joblib"

    parser = argparse.ArgumentParser(
        description="Train/predict a multiclass audit typology classifier from transaction data."
    )
    subparsers = parser.add_subparsers(dest="command", required=True)

    train_parser = subparsers.add_parser("train", help="Train typology classifier")
    train_parser.add_argument("--cases-dir", type=Path, default=pipeline.cases_dir)
    train_parser.add_argument("--patterns-path", type=Path, default=pipeline.patterns_path)
    train_parser.add_argument("--model-output", type=Path, default=default_model_path)
    train_parser.add_argument("--expected-types", type=int, default=10)
    train_parser.add_argument("--test-size", type=float, default=0.2)
    train_parser.add_argument("--random-state", type=int, default=42)
    train_parser.add_argument("--min-transactions", type=int, default=1)
    train_parser.set_defaults(handler=_train_cmd)

    predict_parser = subparsers.add_parser("predict", help="Predict typology for one case JSON")
    predict_parser.add_argument("--case-json", type=Path, required=True)
    predict_parser.add_argument("--model-path", type=Path, default=default_model_path)
    predict_parser.add_argument("--top-k", type=int, default=3)
    predict_parser.set_defaults(handler=_predict_cmd)

    return parser.parse_args()


def main() -> None:
    args = parse_args()
    args.handler(args)


if __name__ == "__main__":
    main()
